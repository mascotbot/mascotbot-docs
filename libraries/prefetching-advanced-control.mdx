---
title: Audio Prefetching & Advanced Control - Optimize Mascot Bot SDK Performance
sidebarTitle: Prefetching & Advanced Control
description: Master audio prefetching and manual state control in Mascot Bot SDK. Essential for video exports, queue management, and high-performance applications. Learn advanced patterns →
'og:title': 'Audio Prefetching & Advanced Control | Mascot Bot SDK'
'og:description': 'Advanced techniques for audio prefetching, manual state control, and queue management in Mascot Bot SDK. Perfect for video exports and performance optimization.'
'twitter:title': 'Master Audio Prefetching in Mascot Bot SDK'
'twitter:description': 'Learn advanced prefetching patterns, manual state control, and performance optimization techniques for Mascot Bot SDK.'
'article:published_time': '2025-01-04T00:00:00+00:00'
'article:author': 'Mascot Bot Team'
'article:section': 'SDK Documentation'
'article:tag': 'prefetching, performance, video export, advanced, audio'
---

# Audio Prefetching & Advanced Control

Unlock the full potential of Mascot Bot SDK with advanced audio prefetching and manual state control. Essential for video exports, multi-scene rendering, and high-performance applications requiring precise timing control.


<Columns cols={2}>
  <Card title="Quick Start" icon="rocket" href="#quick-start">
    Start prefetching in minutes
  </Card>
  <Card title="Use Cases" icon="lightbulb" href="#use-cases">
    Video export, queue management
  </Card>
  <Card title="API Reference" icon="code" href="#api-reference">
    Complete API documentation
  </Card>
  <Card title="Examples" icon="file-code" href="#examples">
    Real-world implementations
  </Card>
</Columns>

## Why Use Prefetching?

Traditional streaming approaches introduce latency between speech segments. Prefetching eliminates these gaps by loading audio and viseme data ahead of time, enabling:

- **Seamless Video Exports**: Pre-load all audio before rendering
- **Smooth Transitions**: Zero delay between sequential speech
- **Offline Playback**: Cache audio for disconnected scenarios
- **Performance Testing**: Compare multiple voices without waiting

## Core Concepts

### Prefetching Architecture

The SDK's prefetching system separates data fetching from playback:

```
1. Fetch Phase: Download audio + viseme data without playing
2. Store Phase: Cache data in memory with timing information
3. Play Phase: Use cached data for instant playback
```

### Manual State Control

When prefetching, you often need direct control over the speaking state, bypassing automatic detection:

```tsx
// Disable automatic state management
const speech = useMascotSpeech({
  disableAutomaticSpeakingState: true
});

// Control speaking state manually
playback.setSpeakingStateManually(true);
```

## Quick Start

### Basic Prefetching

<Note>
The `playAudioFromPrefetchedData` function shown below is not included in the SDK - you need to implement it yourself. See the [complete implementation](#playing-prefetched-audio) in the Examples section.
</Note>

```tsx
import { useMascotSpeech, useMascotPlayback } from '@mascotbot-sdk/react';

function PrefetchExample() {
  const speech = useMascotSpeech({
    apiEndpoint: "/api/visemes-audio",
    disableAutomaticSpeakingState: true // Critical for prefetching
  });

  const playback = useMascotPlayback({
    manualSpeakingStateControl: true
  });

  const handlePrefetchAndPlay = async () => {
    // 1. Prefetch audio data
    const prefetchedData = await speech.prefetchAudio("Hello world", {
      ttsParams: {
        voice: "en-US-1",
        speed: 1.0
      }
    });

    // 2. Load viseme data for lip sync
    playback.loadPrefetchedData(prefetchedData.audioData.visemesBySequence);

    // 3. Manually control speaking state
    playback.setSpeakingStateManually(true);
    playback.play();

    // 4. Play audio from prefetched data
    // NOTE: You must implement this function - see Examples section
    await playAudioFromPrefetchedData(prefetchedData);

    // 5. Reset state when done
    playback.setSpeakingStateManually(false);
    playback.reset();
  };

  return (
    <button onClick={handlePrefetchAndPlay}>
      Prefetch & Play
    </button>
  );
}
```

## API Reference

### useMascotSpeech Options

```typescript
interface MascotSpeechOptions {
  // ... standard options

  // Disable automatic speaking state management
  // Essential for prefetching workflows
  disableAutomaticSpeakingState?: boolean;
}
```

### prefetchAudio Method

```typescript
const prefetchedData = await speech.prefetchAudio(
  text: string,
  options?: {
    ttsParams?: {
      tts_engine?: string;
      voice?: string;
      speed?: number;
      tts_api_key?: string;
    }
  }
): Promise<{
  audioData: {
    audioEvents: Map<number, AudioEvent>;  // Base64-encoded PCM audio chunks
    visemesBySequence: Map<number, VisemeData[]>;  // Viseme timing data
  };
  duration: number;  // Total duration in milliseconds
}>;

// AudioEvent structure
interface AudioEvent {
  data: string;        // Base64-encoded PCM audio data
  sample_rate: number; // Sample rate (e.g., 24000)
}
```

### useMascotPlayback Methods

```typescript
interface MascotPlaybackMethods {
  // Load prefetched viseme data
  loadPrefetchedData(visemeData: Map<number, VisemeData[]>): void;

  // Manual speaking state control
  setSpeakingStateManually(isSpeaking: boolean): void;

  // Standard playback controls
  play(): void;
  pause(): void;
  reset(): void;
}
```

## Use Cases

### Video Export System

The most common use case for prefetching is video export, where all audio must be loaded before rendering begins:

```tsx
async function exportVideo(items: Array<{ text: string, voice: string }>) {
  // 1. Create shared AudioContext for all exports
  const sharedAudioContext = new AudioContext();
  const audioCache = new Map<number, PrefetchedData>();
  
  // 2. Prefetch all audio in parallel
  const prefetchPromises = items.map(async (item, index) => {
    const data = await speech.prefetchAudio(item.text, {
      ttsParams: {
        voice: item.voice,
        speed: 1.0
      }
    });
    audioCache.set(index, data);
  });

  await Promise.all(prefetchPromises);

  // 3. Render video with cached audio
  for (let i = 0; i < items.length; i++) {
    const prefetchedData = audioCache.get(i);
    if (prefetchedData) {
      // Load visemes for lip sync
      playback.loadPrefetchedData(prefetchedData.audioData.visemesBySequence);
      playback.setSpeakingStateManually(true);
      playback.play();
      
      // Play audio
      await playAudioFromPrefetchedData(prefetchedData, sharedAudioContext);
      
      // Reset after playback
      playback.setSpeakingStateManually(false);
      playback.reset();
    }
  }
}
```

### Sequential Speech Queue

Prefetching enables smooth transitions between multiple speech segments:

```tsx
function SpeechQueue({ items }: { items: string[] }) {
  const [queue, setQueue] = useState<PrefetchedData[]>([]);
  
  // Prefetch all items on mount
  useEffect(() => {
    const prefetchAll = async () => {
      const data = await Promise.all(
        items.map(text => speech.prefetchAudio(text))
      );
      setQueue(data);
    };
    prefetchAll();
  }, [items]);

  // Play queue sequentially
  const playQueue = async () => {
    for (const data of queue) {
      playback.loadPrefetchedData(data.audioData.visemesBySequence);
      playback.setSpeakingStateManually(true);
      playback.play();
      
      await playAudioFromPrefetchedData(data);
      
      playback.setSpeakingStateManually(false);
      playback.reset();
      
      // Small gap between items
      await new Promise(resolve => setTimeout(resolve, 100));
    }
  };

  return <button onClick={playQueue}>Play All</button>;
}
```

### Voice Comparison Tool

Prefetch multiple voice options for instant comparison:

```tsx
function VoiceComparison({ text }: { text: string }) {
  const [voiceData, setVoiceData] = useState<Map<string, PrefetchedData>>();
  
  const voices = ['en-US-1', 'en-US-2', 'en-UK-1'];
  
  useEffect(() => {
    const prefetchVoices = async () => {
      const data = new Map();
      
      await Promise.all(
        voices.map(async (voice) => {
          const prefetched = await speech.prefetchAudio(text, {
            ttsParams: { voice }
          });
          data.set(voice, prefetched);
        })
      );
      
      setVoiceData(data);
    };
    
    prefetchVoices();
  }, [text]);

  const playVoice = async (voice: string) => {
    const data = voiceData?.get(voice);
    if (data) {
      playback.loadPrefetchedData(data.audioData.visemesBySequence);
      playback.setSpeakingStateManually(true);
      playback.play();
      
      await playAudioFromPrefetchedData(data);
      
      playback.setSpeakingStateManually(false);
      playback.reset();
    }
  };

  return (
    <div>
      {voices.map(voice => (
        <button key={voice} onClick={() => playVoice(voice)}>
          Play {voice}
        </button>
      ))}
    </div>
  );
}
```

## Examples

### Playing Prefetched Audio

<Warning>
The `playAudioFromPrefetchedData` function is NOT included in the SDK. You must implement it yourself using the Web Audio API. Below is the exact implementation used in the Mascot Bot app for video exports.
</Warning>

<Info>
**Why isn't this in the SDK?** The SDK focuses on real-time streaming use cases. Prefetching is an advanced pattern where you may want custom control over audio playback timing, audio context management, and integration with your app's audio system. By implementing this yourself, you have full control over these aspects.
</Info>

#### Complete Implementation

Here's a production-ready implementation for playing prefetched audio:

```tsx
/**
 * Create audio buffer from pre-fetched audio data
 */
async function createAudioBufferFromData(
  audioData: any,
  audioContext: AudioContext
): Promise<AudioBuffer | null> {
  try {
    // Check if we have valid audio data
    if (!audioData || !audioData.audioEvents || audioData.audioEvents.size === 0) {
      console.warn('[Export] No audio events found in audio data');
      return null;
    }

    const audioBuffers: AudioBuffer[] = [];
    
    // Process each audio event in the Map
    for (const [sequence, audioEvent] of audioData.audioEvents) {
      if (!audioEvent.data) {
        console.warn(`[Export] No data in audio event sequence ${sequence}`);
        continue;
      }

      // Decode base64 to binary
      const binaryString = atob(audioEvent.data);
      const bytes = new Uint8Array(binaryString.length);
      for (let i = 0; i < binaryString.length; i++) {
        bytes[i] = binaryString.charCodeAt(i);
      }
      
      const samplesCount = bytes.length / 2;
      
      // Skip if no samples
      if (samplesCount === 0) {
        console.warn(`[Export] Audio event sequence ${sequence} has no samples`);
        continue;
      }
      
      // Convert bytes to Int16 PCM (little-endian)
      const int16Array = new Int16Array(samplesCount);
      for (let i = 0; i < samplesCount; i++) {
        const low = bytes[i * 2];
        const high = bytes[i * 2 + 1];
        int16Array[i] = (high << 8) | low;
      }
      
      // Create AudioBuffer with proper sample rate
      const sampleRate = audioEvent.sample_rate || 44100;
      const audioBuffer = audioContext.createBuffer(1, int16Array.length, sampleRate);
      const channelData = audioBuffer.getChannelData(0);
      
      // Convert Int16 to Float32 (-1.0 to 1.0 range)
      for (let i = 0; i < int16Array.length; i++) {
        channelData[i] = int16Array[i] / 32768.0;
      }
      
      audioBuffers.push(audioBuffer);
    }
    
    // Check if we have any valid buffers
    if (audioBuffers.length === 0) {
      console.warn('[Export] No valid audio buffers created from audio events');
      return null;
    }
    
    // Combine all audio buffers into one
    const totalLength = audioBuffers.reduce((acc, buf) => acc + buf.length, 0);
    
    if (totalLength === 0) {
      console.warn('[Export] Total audio buffer length is 0');
      return null;
    }
    
    const sampleRate = audioBuffers[0]?.sampleRate || 44100;
    const combinedBuffer = audioContext.createBuffer(1, totalLength, sampleRate);
    const combinedChannelData = combinedBuffer.getChannelData(0);
    
    let offset = 0;
    for (const buffer of audioBuffers) {
      combinedChannelData.set(buffer.getChannelData(0), offset);
      offset += buffer.length;
    }
    
    return combinedBuffer;
  } catch (error) {
    console.error('[Export] Failed to create audio buffer:', error);
    return null;
  }
}

/**
 * Play audio from prefetched data
 */
async function playAudioFromPrefetchedData(
  prefetchedData: PreFetchedData,
  audioContext?: AudioContext
) {
  // Create or reuse AudioContext
  const ctx = audioContext || new AudioContext();
  
  // Ensure context is running
  if (ctx.state === 'suspended') {
    await ctx.resume();
  }
  
  // Convert prefetched data to audio buffer
  const audioBuffer = await createAudioBufferFromData(
    prefetchedData.audioData,
    ctx
  );
  
  if (!audioBuffer) {
    throw new Error('Failed to create audio buffer');
  }
  
  // Create and play audio source
  const source = ctx.createBufferSource();
  source.buffer = audioBuffer;
  source.connect(ctx.destination);
  
  // Return promise that resolves when audio ends
  return new Promise<void>((resolve) => {
    source.onended = () => resolve();
    source.start(0);
  });
}
```

### Simple Audio Playback Implementation

If you need a simpler version without error handling:

```tsx
// Minimal implementation for quick testing
async function playAudioFromPrefetchedData(prefetchedData: any) {
  const audioContext = new AudioContext();
  
  // Get first audio event (simplified - production code should handle all events)
  const firstAudioEvent = prefetchedData.audioData.audioEvents.get(0);
  if (!firstAudioEvent) return;
  
  // Decode base64 to ArrayBuffer
  const binaryString = atob(firstAudioEvent.data);
  const bytes = new Uint8Array(binaryString.length);
  for (let i = 0; i < binaryString.length; i++) {
    bytes[i] = binaryString.charCodeAt(i);
  }
  
  // Convert to Float32 audio data
  const audioBuffer = audioContext.createBuffer(
    1, // mono
    bytes.length / 2, // 16-bit PCM
    firstAudioEvent.sample_rate
  );
  
  const channelData = audioBuffer.getChannelData(0);
  for (let i = 0; i < bytes.length / 2; i++) {
    const int16 = (bytes[i * 2 + 1] << 8) | bytes[i * 2];
    channelData[i] = int16 / 32768.0;
  }
  
  // Play the audio
  const source = audioContext.createBufferSource();
  source.buffer = audioBuffer;
  source.connect(audioContext.destination);
  source.start();
  
  // Wait for playback to complete
  return new Promise(resolve => {
    source.onended = resolve;
  });
}
```

### Advanced Export with Progress Tracking

```tsx
function ExportDialog({ scenes }: { scenes: Scene[] }) {
  const [progress, setProgress] = useState(0);
  const [status, setStatus] = useState<'idle' | 'prefetching' | 'rendering'>('idle');

  const speech = useMascotSpeech({
    disableAutomaticSpeakingState: true
  });

  const handleExport = async () => {
    setStatus('prefetching');
    
    // Track prefetch progress
    let completed = 0;
    const total = scenes.filter(s => s.voiceover?.text).length;
    
    const prefetchPromises = scenes.map(async (scene, index) => {
      if (scene.voiceover?.text) {
        const data = await speech.prefetchAudio(scene.voiceover.text);
        completed++;
        setProgress((completed / total) * 50); // First 50% for prefetching
        return { scene, data };
      }
      return null;
    });

    const prefetchedScenes = await Promise.all(prefetchPromises);
    
    setStatus('rendering');
    
    // Render with progress tracking
    for (let i = 0; i < prefetchedScenes.length; i++) {
      const item = prefetchedScenes[i];
      if (item) {
        await renderScene(item.scene, item.data);
        setProgress(50 + ((i + 1) / prefetchedScenes.length) * 50);
      }
    }
    
    setStatus('idle');
  };

  return (
    <div>
      <button onClick={handleExport} disabled={status !== 'idle'}>
        Export Video
      </button>
      {status !== 'idle' && (
        <div>
          <p>Status: {status}</p>
          <progress value={progress} max={100} />
        </div>
      )}
    </div>
  );
}
```

## Best Practices

### 1. Always Disable Automatic State Management

When using prefetching, always disable automatic speaking state detection:

```tsx
// ✅ Correct
const speech = useMascotSpeech({
  disableAutomaticSpeakingState: true
});

// ❌ Incorrect - will cause conflicts
const speech = useMascotSpeech({});
```

### 2. Reuse AudioContext

Create a single AudioContext and reuse it across all prefetched audio playback:

```tsx
// ✅ Correct - single context
const audioContext = new AudioContext();

for (const data of prefetchedItems) {
  await playAudioFromPrefetchedData(data, audioContext);
}

// ❌ Incorrect - multiple contexts
for (const data of prefetchedItems) {
  const ctx = new AudioContext(); // Creates new context each time
  await playAudioFromPrefetchedData(data, ctx);
}
```

### 3. Handle Errors Gracefully

Always implement error handling for prefetch operations:

```tsx
const prefetchWithRetry = async (text: string, maxRetries = 3) => {
  for (let attempt = 0; attempt < maxRetries; attempt++) {
    try {
      return await speech.prefetchAudio(text);
    } catch (error) {
      if (attempt === maxRetries - 1) throw error;
      await new Promise(resolve => setTimeout(resolve, 1000 * (attempt + 1)));
    }
  }
};
```

### 4. Clean Up Resources

Always reset playback state after use:

```tsx
try {
  playback.loadPrefetchedData(data.visemesBySequence);
  playback.setSpeakingStateManually(true);
  playback.play();
  
  await playAudioFromPrefetchedData(data);
} finally {
  // Always clean up
  playback.setSpeakingStateManually(false);
  playback.reset();
  speech.stopAndClear();
}
```

## Performance Considerations

### Memory Management

Prefetching stores audio data in memory. For large projects:

```tsx
// Clear cached data when no longer needed
const audioCache = new Map();

// After use
audioCache.clear();
```

### Parallel vs Sequential Prefetching

```tsx
// ✅ Parallel - faster for multiple items
const allData = await Promise.all(
  items.map(item => speech.prefetchAudio(item))
);

// ❌ Sequential - slower but uses less memory
const allData = [];
for (const item of items) {
  allData.push(await speech.prefetchAudio(item));
}
```

### Browser Limits

Be aware of browser AudioContext limits (typically 6 simultaneous contexts):

```tsx
// Monitor active contexts
let activeContexts = 0;
const MAX_CONTEXTS = 6;

if (activeContexts < MAX_CONTEXTS) {
  const ctx = new AudioContext();
  activeContexts++;
  
  ctx.addEventListener('statechange', () => {
    if (ctx.state === 'closed') {
      activeContexts--;
    }
  });
}
```

## Troubleshooting

### Common Issues

<AccordionGroup>
  <Accordion title="Audio plays but no lip sync">
    Ensure you're loading viseme data before playing:
    ```tsx
    // Load viseme data first
    playback.loadPrefetchedData(data.audioData.visemesBySequence);
    // Then control speaking state
    playback.setSpeakingStateManually(true);
    ```
  </Accordion>

  <Accordion title="Speaking state conflicts">
    Make sure automatic state management is disabled:
    ```tsx
    const speech = useMascotSpeech({
      disableAutomaticSpeakingState: true
    });
    ```
  </Accordion>

  <Accordion title="Memory leaks with large exports">
    Clear prefetched data after use:
    ```tsx
    // Clear individual items
    audioCache.delete(sceneId);
    
    // Clear entire cache
    audioCache.clear();
    ```
  </Accordion>
</AccordionGroup>