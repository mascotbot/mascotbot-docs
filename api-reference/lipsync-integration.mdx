---
title: "Integrating Lipsync API with Any TTS for Mascot Animation"
sidebarTitle: "Integrating TTS & Lipsync"
description: "Learn how to connect any TTS to our lipsync API for animating mascots."
icon: 'lips'
---

In this guide, we'll explore how developers can use our lipsync endpoints to animate mascots with synchronized speech. We offer two approaches: a simplified one-call solution and a flexible two-step process for custom TTS integration.

## Two Integration Approaches

### ðŸš€ Approach 1: Simplified One-Call Integration (Recommended)
Use our `/v1/visemes-audio` endpoint to generate speech and visemes in a single streaming API call. Supports ElevenLabs, Cartesia, and our built-in TTS engine.

### ðŸ”§ Approach 2: Custom TTS Integration  
Use any TTS service with our `/v1/visemes` endpoint for maximum flexibility when you have specific TTS requirements.

## Approach 1: Simplified Integration with Built-in TTS

This is the recommended approach for most use cases. The `/v1/visemes-audio` endpoint handles both text-to-speech conversion and viseme generation in a single streaming call.

<iframe
  src="https://mascotbot-examples.vercel.app/voice-over"
  width="100%"
  height="700"
  style={{ border: "none", borderRadius: "8px", overflow: "auto", touchAction: "pan-y" }}
  title="Mascotbot Voice Over Example"
  allow="autoplay; encrypted-media; microphone; clipboard-write"
></iframe>

### Server-Side Implementation (Simplified)

```typescript
import { NextRequest, NextResponse } from "next/server";

const MASCOTBOT_API_KEY = "your-mascotbot-api-key";
const ELEVENLABS_API_KEY = "your-elevenlabs-api-key"; // Optional, for ElevenLabs voices

export async function POST(req: NextRequest) {
  try {
    const { text, voice = "am_fenrir", tts_engine = "mascotbot" } = await req.json();

    // Single API call handles both TTS and viseme generation
    const response = await fetch("https://api.mascot.bot/v1/visemes-audio", {
      method: "POST",
      body: JSON.stringify({
        text,
        voice,
        tts_engine,
        tts_api_key: tts_engine === "elevenlabs" ? ELEVENLABS_API_KEY : undefined,
        speed: 1.0
      }),
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${MASCOTBOT_API_KEY}`,
      },
    });

    // Stream the response directly to the client
    return new Response(response.body, {
      headers: {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
      },
    });
  } catch (error) {
    return NextResponse.json({ message: "Internal Server Error" }, { status: 500 });
  }
}
```

### Client-Side Implementation (Simplified)

```typescript
import { useRef, useState } from "react";
import { useMascotPlayback } from "@mascotbot-sdk/react";
import { Button, Textarea } from "@mascotbot/ui";

export default function StreamingVoiceOver() {
  const mascotPlayback = useMascotPlayback();
  const [text, setText] = useState<string>("");
  const audioRef = useRef<HTMLAudioElement>(null);
  const [isPlaying, setIsPlaying] = useState(false);

  const handleStreamingPlayback = async () => {
    setIsPlaying(true);
    
    const response = await fetch("/api/streaming-voice-over", {
      method: "POST",
      body: JSON.stringify({ text }),
    });

    const reader = response.body?.getReader();
    const decoder = new TextDecoder();
    
    let audioChunks: string[] = [];
    let isFirstAudio = true;

    while (true) {
      const { done, value } = await reader!.read();
      if (done) break;

      const chunk = decoder.decode(value);
      const lines = chunk.split('\n');

      for (const line of lines) {
        if (line.startsWith('data: ')) {
          const data = JSON.parse(line.slice(6));
          
          if (data.type === 'audio') {
            audioChunks.push(data.data);
            
            // Start playback with first audio chunk
            if (isFirstAudio) {
              const audioBlob = new Blob([new Uint8Array(atob(data.data).split('').map(c => c.charCodeAt(0)))]);
              const audioUrl = URL.createObjectURL(audioBlob);
              audioRef.current!.src = audioUrl;
              audioRef.current!.play();
              isFirstAudio = false;
            }
          } else if (data.type === 'visemes') {
            // Add visemes to mascot animation
            mascotPlayback.add(data.visemes);
            if (!mascotPlayback.isPlaying) {
              mascotPlayback.play();
            }
          }
        }
      }
    }
    
    setIsPlaying(false);
  };

  return (
    <div>
      <audio ref={audioRef} playsInline />
      <Textarea value={text} onChange={({ target }) => setText(target.value)} />
      <Button onClick={handleStreamingPlayback} disabled={isPlaying}>
        {isPlaying ? "Playing..." : "Play with Streaming"}
      </Button>
    </div>
  );
}
```

## Approach 2: Custom TTS Integration

Use this approach when you need maximum flexibility with your TTS provider or want to use a service not directly supported by our built-in integration.

### How this approach works

Here's the data flow for custom TTS integration:

1. **Client-Side**: The user inputs text, which is sent to the server.
2. **Server-Side**: The server processes the text using your chosen TTS service to generate audio and sends it to the `/v1/visemes` endpoint to get visemes.
3. **Client-Side**: The audio and visemes are sent back to the client, where they are used to animate the mascot.

<img
  className="block dark:hidden"
  src="/images/lipsync_light.svg"
  alt="Lipsync Light"
/>
<img
  className="hidden dark:block"
  src="/images/lipsync_dark.svg"
  alt="Lipsync Dark"
/>

### Server Route Implementation

```typescript
import { NextRequest, NextResponse } from "next/server";
import { ElevenLabsClient } from "elevenlabs";
import { createWavBufferFromPCM } from "./utils";

const ELEVENLABS_API_KEY = "your-elevenlabs-api-key";
const VOICE_ID = "your-voice-id";
const MASCOTBOT_API_KEY = "your-mascotbot-api-key";

const elevenlabs = new ElevenLabsClient({
  apiKey: ELEVENLABS_API_KEY,
});

export async function POST(req: NextRequest) {
  try {
    const { text } = await req.json();

    // Step 1: Convert text to speech using your chosen TTS service
    const rawPcmBase64 = (await elevenlabs.textToSpeech.convertWithTimestamps(VOICE_ID, {
      output_format: "pcm_16000",
      text,
    })) as { audio_base64: string };

    const pcmBuffer = Buffer.from(rawPcmBase64.audio_base64, "base64");
    const wavBuffer = createWavBufferFromPCM(pcmBuffer, 16000, 16, 1);

    // Step 2: Send the audio to MascotBot API for viseme generation
    const response = await fetch("https://api.mascot.bot/v1/visemes", {
      method: "POST",
      body: JSON.stringify({
        audio: wavBuffer.toString("base64"),
        sample_rate: 16000,
      }),
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${MASCOTBOT_API_KEY}`,
      },
    });

    const visemes = await response.json();

    return NextResponse.json({
      audio: wavBuffer.toString("base64"),
      visemes,
    });
  } catch (error) {
    return NextResponse.json({ message: "Internal Server Error" }, { status: 500 });
  }
}
```

### Client-Side Integration

```typescript
import { useRef, useState } from "react";
import { useMascotPlayback } from "@mascotbot-sdk/react";
import { Button, Textarea } from "@mascotbot/ui";

const API_DOMAIN = "https://your-api-domain.com";

export default function CustomTTSVoiceOver() {
  const mascotPlayback = useMascotPlayback();
  const [text, setText] = useState<string>("");
  const audioRef = useRef<HTMLAudioElement>(null);

  return (
    <div>
      <audio ref={audioRef} playsInline />
      <Textarea value={text} onChange={({ target }) => setText(target.value)} />
      <Button
        onClick={async () => {
          // Send text to server for TTS + viseme processing
          const response = await fetch(`${API_DOMAIN}/api/custom-tts-voice-over`, {
            method: "POST",
            body: JSON.stringify({ text }),
          });

          const { audio, visemes } = await response.json();

          // Convert base64 audio to playable format
          const audioBlob = new Blob([audio], { type: "audio/wav" });
          const audioUrl = URL.createObjectURL(audioBlob);

          // Setup synchronized playback
          mascotPlayback.add(visemes);

          if (audioRef.current) {
            audioRef.current.src = audioUrl;
            audioRef.current.oncanplay = () => {
              mascotPlayback.play();
              audioRef.current.play();
            };
            audioRef.current.onended = () => mascotPlayback.reset();
          }
        }}
      >
        Play with Custom TTS
      </Button>
    </div>
  );
}
```

## Choosing the Right Approach

**Use Approach 1 (Simplified)** when:
- You want the fastest development experience
- Built-in voices meet your requirements  
- You want real-time streaming for better UX
- You're using supported TTS engines (ElevenLabs, Cartesia, MascotBot)

**Use Approach 2 (Custom TTS)** when:
- You have specific TTS requirements not covered by built-in engines
- You need complete control over audio processing
- You're integrating with proprietary or specialized TTS systems
- You have existing TTS infrastructure you want to maintain

<Note>
Both approaches support the same viseme output format and work with all MascotBot SDKs. The streaming approach (Approach 1) provides better user experience with lower latency.
</Note>

<Tip>
You'll get **working examples** for both approaches once subscribed to one of our paid plans.
</Tip>